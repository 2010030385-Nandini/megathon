{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4246cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e87aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1337\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_RATIO = 0.1\n",
    "MODEL_NAME = \"accent_recognition\"\n",
    "\n",
    "# Location where the dataset will be downloaded.\n",
    "# By default (None), keras.utils.get_file will use ~/.keras/ as the CACHE_DIR\n",
    "CACHE_DIR = None\n",
    "\n",
    "# The location of the dataset\n",
    "URL_PATH = \"C:\\\\Users\\\\vidya\\\\OneDrive\\\\Desktop\\\\train\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets compressed files that contain the audio files\n",
    "zip_files = {\n",
    "    0: \"afrikaans\",\n",
    "    1: \"albanian\",\n",
    "    2: \"amazigh\",\n",
    "    3: \"amharic\",\n",
    "    4: \"arabic\",\n",
    "    5: \"armenian\",\n",
    "    6: \"azerbaijani\",\n",
    "    7: \"bafang\",\n",
    "    8: \"bamabra\",\n",
    "    9: \"bari\",\n",
    "    10: \"basque\",\n",
    "    11: \"bavarian\",\n",
    "    12: \"belarusan\",\n",
    "    13: \"bengali\",\n",
    "    14: \"bosnian\",\n",
    "    15: \"bulgarian\",\n",
    "    16: \"burmese\",\n",
    "    17: \"cantonese\",\n",
    "    18: \"catalan\",\n",
    "    19: \"chaldean\",\n",
    "    20: \"croatian\",\n",
    "    21: \"czech\"\n",
    "    22: \"danish\"\n",
    "    23: \"dari\"\n",
    "    24: \"dutch\"\n",
    "    25: \"english\"\n",
    "    26: \"estonian\"\n",
    "    27: \"ewe\"\n",
    "    28: \"fanti\"\n",
    "    29: \"farsi\"\n",
    "    30: \"fijian\"\n",
    "    31: \"filipino\"\n",
    "    32: \"finnish\"\n",
    "    33:\"french\"\n",
    "    34:\"ga\"\n",
    "    35:\"ganda\"\n",
    "    36:\"garifuna\"\n",
    "    37:\"georgian\"\n",
    "    38:\"german\"\n",
    "    39:\"greek\"\n",
    "    40:\"gujrati\"\n",
    "    41:\"gusii\"\n",
    "    42:\"hadiyya\"\n",
    "    43:\"hausa\"\n",
    "    44:\"hebrew\"\n",
    "    45:\"hindi\"\n",
    "    46:\"hmong\"\n",
    "    47:\"hugarian\"\n",
    "    48:\"ibibiu\"\n",
    "    49:\"icelandic\"\n",
    "    50:\"igbo\"\n",
    "    51:\"indonesian\"\n",
    "    52:\"italian\"\n",
    "    53:\"japanese\"\n",
    "    54:\"kambaata\"\n",
    "    55:\"kazakh\"\n",
    "    56:\"khmer\"\n",
    "    57:\"kikongo\"\n",
    "    58:\"kikuyu\"\n",
    "    59:\"kiswahili\"\n",
    "    60:\"korean\"\n",
    "    61:\"krio\"\n",
    "    62:\"kurdish\"\n",
    "    63:\"lao\"\n",
    "    64:\"latvian\"\n",
    "    65:\"lithuanian\"\n",
    "    66:\"luo\"\n",
    "    67:\"macedonian\"\n",
    "    68:\"malay\"\n",
    "    69:\"malayalam\"\n",
    "    70:\"maltese\"\n",
    "    71:\"mandarin\"\n",
    "    72:\"marathi\"\n",
    "    73:\"mauritian\"\n",
    "    74:\"mende\"\n",
    "    75:\"miskito\"\n",
    "    76:\"mongolian\"\n",
    "    77:\"nepali\"\n",
    "    78:\"ngemba\"\n",
    "    79:\"norwegian\"\n",
    "    80:\"oriya\"\n",
    "    81:\"oromo\"\n",
    "    82:\"papiamentu\"\n",
    "    83:\"pashto\"\n",
    "    84:\"polish\"\n",
    "    85:\"portuguese\"\n",
    "    86:\"pulaar\"\n",
    "    87:\"punjabi\"\n",
    "    88:\"quechua\"\n",
    "    89:\"romanian\"\n",
    "    90:\"rotuman\"\n",
    "    91:\"russian\"\n",
    "    92:\"satawalese\"\n",
    "    93:\"serbian\"\n",
    "    94:\"shona\"\n",
    "    95:\"slovak\"\n",
    "    96:\"slovenian\"\n",
    "    97:\"somali\"\n",
    "    98:\"spanish\"\n",
    "    99:\"swedish\"\n",
    "    100:\"synphesized\"\n",
    "    101:\"tagalog\"\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "gender_agnostic_categories = [\n",
    "    \"ir\",  \n",
    "    \"mi\", \n",
    "    \"no\",  \n",
    "    \"sc\", \n",
    "    \"so\",  \n",
    "    \"we\",  \n",
    "]\n",
    "\n",
    "class_names = [\n",
    "    \"Irish\",\n",
    "    \"Midlands\",\n",
    "    \"Northern\",\n",
    "    \"Scottish\",\n",
    "    \"Southern\",\n",
    "    \"Welsh\",\n",
    "    \"Not a speech\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "# Set all random seeds in order to get reproducible results\n",
    "keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# Where to download the dataset\n",
    "DATASET_DESTINATION = os.path.join(CACHE_DIR if CACHE_DIR else \"~/.keras/\", \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ffe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_index_file = keras.utils.get_file(\n",
    "    fname=\"line_index_file\", origin=URL_PATH + \"line_index_all.csv\"\n",
    ")\n",
    "\n",
    "for i in zip_files:\n",
    "    fname = zip_files[i].split(\".\")[0]\n",
    "    url = URL_PATH + zip_files[i]\n",
    "\n",
    "    zip_file = keras.utils.get_file(fname=fname, origin=url, extract=True)\n",
    "    os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\n",
    "    line_index_file, names=[\"id\", \"filename\", \"transcript\"], usecols=[\"filename\"]\n",
    ")\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368996ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(dataframe):\n",
    "    # Remove leading space in filename column\n",
    "    dataframe[\"filename\"] = dataframe.apply(lambda row: row[\"filename\"].strip(), axis=1)\n",
    "\n",
    "    # Create gender agnostic labels based on the filename first 2 letters\n",
    "    dataframe[\"label\"] = dataframe.apply(\n",
    "        lambda row: gender_agnostic_categories.index(row[\"filename\"][:2]), axis=1\n",
    "    )\n",
    "\n",
    "    # Add the file path to the name\n",
    "    dataframe[\"filename\"] = dataframe.apply(\n",
    "        lambda row: os.path.join(DATASET_DESTINATION, row[\"filename\"] + \".wav\"), axis=1\n",
    "    )\n",
    "\n",
    "    # Shuffle the samples\n",
    "    dataframe = dataframe.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "dataframe = preprocess_dataframe(dataframe)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d254708",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(dataframe) * (1 - VALIDATION_RATIO))\n",
    "train_df = dataframe[:split]\n",
    "valid_df = dataframe[split:]\n",
    "\n",
    "print(\n",
    "    f\"We have {train_df.shape[0]} training samples & {valid_df.shape[0]} validation ones\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_16k_audio_wav(filename):\n",
    "    # Read file content\n",
    "    file_content = tf.io.read_file(filename)\n",
    "\n",
    "    # Decode audio wave\n",
    "    audio_wav, sample_rate = tf.audio.decode_wav(file_content, desired_channels=1)\n",
    "    audio_wav = tf.squeeze(audio_wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "\n",
    "    # Resample to 16k\n",
    "    audio_wav = tfio.audio.resample(audio_wav, rate_in=sample_rate, rate_out=16000)\n",
    "\n",
    "    return audio_wav\n",
    "\n",
    "\n",
    "def filepath_to_embeddings(filename, label):\n",
    "    # Load 16k audio wave\n",
    "    audio_wav = load_16k_audio_wav(filename)\n",
    "\n",
    "    # Get audio embeddings & scores.\n",
    "    # The embeddings are the audio features extracted using transfer learning\n",
    "    # while scores will be used to identify time slots that are not speech\n",
    "    # which will then be gathered into a specific new category 'other'\n",
    "    scores, embeddings, _ = yamnet_model(audio_wav)\n",
    "\n",
    "    # Number of embeddings in order to know how many times to repeat the label\n",
    "    embeddings_num = tf.shape(embeddings)[0]\n",
    "    labels = tf.repeat(label, embeddings_num)\n",
    "\n",
    "    # Change labels for time-slots that are not speech into a new category 'other'\n",
    "    labels = tf.where(tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1)\n",
    "\n",
    "    # Using one-hot in order to use AUC\n",
    "    return (embeddings, tf.one_hot(labels, len(class_names)))\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(dataframe, batch_size=64):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe[\"filename\"], dataframe[\"label\"])\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: filepath_to_embeddings(x, y),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    ).unbatch()\n",
    "\n",
    "    return dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_df)\n",
    "valid_ds = dataframe_to_dataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf65aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def build_and_compile_model():\n",
    "    inputs = keras.layers.Input(shape=(1024), name=\"embedding\")\n",
    "\n",
    "    x = keras.layers.Dense(256, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = keras.layers.Dropout(0.15, name=\"dropout_1\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(192, activation=\"relu\", name=\"dense_3\")(x)\n",
    "    x = keras.layers.Dropout(0.25, name=\"dropout_3\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)\n",
    "    x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"ouput\")(\n",
    "        x\n",
    "    )\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"accent_recognition\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1.9644e-5),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_and_compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9876f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = tf.zeros(shape=(len(class_names),), dtype=tf.int32)\n",
    "\n",
    "for x, y in iter(train_ds):\n",
    "    class_counts = class_counts + tf.math.bincount(\n",
    "        tf.cast(tf.math.argmax(y, axis=1), tf.int32), minlength=len(class_names)\n",
    "    )\n",
    "\n",
    "class_weight = {\n",
    "    i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()\n",
    "    for i in range(len(class_counts))\n",
    "}\n",
    "\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db75bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\", patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    MODEL_NAME + \".h5\", monitor=\"val_auc\", save_best_only=True\n",
    ")\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "    os.path.join(os.curdir, \"logs\", model.name)\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "axs[0].plot(range(EPOCHS), history.history[\"accuracy\"], label=\"Training\")\n",
    "axs[0].plot(range(EPOCHS), history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_title(\"Training & Validation Accuracy\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(range(EPOCHS), history.history[\"auc\"], label=\"Training\")\n",
    "axs[1].plot(range(EPOCHS), history.history[\"val_auc\"], label=\"Validation\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_title(\"Training & Validation AUC\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e782eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, train_auc = model.evaluate(train_ds)\n",
    "valid_loss, valid_acc, valid_auc = model.evaluate(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, train_auc = model.evaluate(train_ds)\n",
    "valid_loss, valid_acc, valid_auc = model.evaluate(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60566497",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = None\n",
    "y_valid = None\n",
    "\n",
    "for x, y in iter(valid_ds):\n",
    "    if x_valid is None:\n",
    "        x_valid = x.numpy()\n",
    "        y_valid = y.numpy()\n",
    "    else:\n",
    "        x_valid = np.concatenate((x_valid, x.numpy()), axis=0)\n",
    "        y_valid = np.concatenate((y_valid, y.numpy()), axis=0)\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(\n",
    "    np.argmax(y_valid, axis=1), np.argmax(y_pred, axis=1)\n",
    ")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    confusion_mtx, xticklabels=class_names, yticklabels=class_names, annot=True, fmt=\"g\"\n",
    ")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(class_names):\n",
    "    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n",
    "    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n",
    "    print(\n",
    "        \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(\n",
    "            label, precision * 100, recall * 100\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(class_names):\n",
    "    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n",
    "    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n",
    "    print(\n",
    "        \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(\n",
    "            label, precision * 100, recall * 100\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yamnet_class_names_from_csv(yamnet_class_map_csv_text):\n",
    "    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
    "    yamnet_class_map_csv = io.StringIO(yamnet_class_map_csv_text)\n",
    "    yamnet_class_names = [\n",
    "        name for (class_index, mid, name) in csv.reader(yamnet_class_map_csv)\n",
    "    ]\n",
    "    yamnet_class_names = yamnet_class_names[1:]  # Skip CSV header\n",
    "    return yamnet_class_names\n",
    "\n",
    "\n",
    "yamnet_class_map_path = yamnet_model.class_map_path().numpy()\n",
    "yamnet_class_names = yamnet_class_names_from_csv(\n",
    "    tf.io.read_file(yamnet_class_map_path).numpy().decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_number_of_non_speech(scores):\n",
    "    number_of_non_speech = tf.math.reduce_sum(\n",
    "        tf.where(tf.math.argmax(scores, axis=1, output_type=tf.int32) != 0, 1, 0)\n",
    "    )\n",
    "\n",
    "    return number_of_non_speech\n",
    "\n",
    "\n",
    "def filename_to_predictions(filename):\n",
    "    audio_wav = load_16k_audio_wav(filename)\n",
    "\n",
    "    scores, embeddings, mel_spectrogram = yamnet_model(audio_wav)\n",
    "\n",
    "    print(\n",
    "        \"Out of {} samples, {} are not speech\".format(\n",
    "            scores.shape[0], calculate_number_of_non_speech(scores)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(embeddings)\n",
    "\n",
    "    return audio_wav, predictions, mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_wav, predictions, mel_spectrogram = filename_to_predictions(filename)\n",
    "\n",
    "infered_class = class_names[predictions.mean(axis=0).argmax()]\n",
    "print(f\"The main accent is: {infered_class} English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f74af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_wav, rate=16000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(audio_wav)\n",
    "plt.xlim([0, len(audio_wav)])\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(\n",
    "    mel_spectrogram.numpy().T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\"\n",
    ")\n",
    "\n",
    "mean_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "top_class_indices = np.argsort(mean_predictions)[::-1]\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(\n",
    "    predictions[:, top_class_indices].T,\n",
    "    aspect=\"auto\",\n",
    "    interpolation=\"nearest\",\n",
    "    cmap=\"gray_r\",\n",
    ")\n",
    "\n",
    "patch_padding = (0.025 / 2) / 0.01\n",
    "plt.xlim([-patch_padding - 0.5, predictions.shape[0] + patch_padding - 0.5])\n",
    "yticks = range(0, len(class_names), 1)\n",
    "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
    "_ = plt.ylim(-0.5 + np.array([len(class_names), 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
